# RL 2016 SS

## Q1 - Value Iteration and Policy Iteration

### a) Value Iteration

Given rewards for the 3x4 gridworld:

| - | - | - | - |
| --- | --- | --- | --- |
| $0$ | $0$ | $0$ | $1$ |
| $0$ | x | $0$ | $-1$ |
| $0$ | $0$ | $0$ | $0$ |

The update rule for value iteration is:

$$ \begin{align} V = (I - \gamma \mathcal{P})^{-1} \mathcal{R} \end{align} $$

Which originates from the Bellman equation
$$ \begin{align} v_\pi(s) = \sum_a \pi(a|s) \sum_{s'} p(s',r|s,a)(r + \gamma v_\pi) \end{align} $$

#### 1st Iteration

| - | - | - | - |
| --- | --- | --- | --- |
| $0$ | $0$ | $\frac{1}{3}$ | $1$ |
| $0$ | x | $-\frac{1}{3}$ | $-1$ |
| $0$ | $0$ | $0$ | $-\frac{1}{3}$ |

A better policy instead of the random policy is the greedy policy 
$$ \begin{align} \pi(s) &= \argmax_a \sum_{s',r} p(s',r|s,a)(r + \gamma v(s')) \\
= \argmax_a (r + v(s')) \end{align} $$

| - | - | - | - |
| --- | --- | --- | --- |
| $0$ | $0$ | $1$ | $1$ |
| $0$ | x | $0$ | $-1$ |
| $0$ | $0$ | $0$ | $0$ |


### b) Policy Iteration

#### i) Update Formula

```
while not done
    policy evaluation
    policy improvement
```

#### ii) Policy Evaluation

Assuming $\pi_0 = \text{right}$ 

##### First Iteration


| $\pi_0$ | - | - | - |
| --- | --- | --- | --- |
| $0$ | $0$ | $1$ | $1$ |
| $0$ | x | $-1$ | $-1$ |
| $0$ | $0$ | $0$ | $0$ |

##### Second Iteration

| - | - | - | - |
| --- | --- | --- | --- |
| &rarr; | &rarr; | &rarr; | $0$ |
| &rarr; | x | &uarr; | $-1$ |
| &rarr; | &rarr; | &rarr; | &rarr; |


## Q2 - Monte Carlo

Fill in the calculations for $Q$ given the episodes.

Sarsa update rule
$$ \begin{align} Q(s,a) = Q(s, a) + \alpha \left( G_t - Q(s,a) \right) \end{align} $$

* $Q((3, 2), T) =50$


## Q3 - Q-Learning

simple

## Q5

* F BUT ACTUALLY T (I think) ((But horrible Q))
* T
* F
* F

### Part 2

* F
* F
* T
* F



