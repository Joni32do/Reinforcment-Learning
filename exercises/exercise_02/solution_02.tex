\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{bbm}
\title{Reinforcement Learning \\ Exercise 2 - Solution}
\author{Jonathan Schnitzler - st166934 \\
ErickVillanuevaVillasenor - st190300 \\
Eric Choquet - st160996}
\date{\today}
\begin{document}
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%
%    Problem 1
%%%%%%%%%%%%%%%%%%%%%%%

\section{Formulating Problems}

\subsection*{a) The game of chess}

\paragraph*{States}
The position of all pieces on the board. A chess board is a 8x8 grid, which starts with 16 white and 16 black pieces on opposing sites. The state space is large (an upper bound from around $\approx 10^{45}$ \footnote{see \url{https://tromp.github.io/chess/chess.html}}).

\paragraph*{Actions}
The possible moves of the current player. The number of possible moves is limited by the number of pieces on the board and the rules of chess.

\paragraph*{Reward Signal}
\begin{itemize}
    \item win, lose or draw the game (by checkmate)
    \item evaluate the current position of the board (e.g. material advantage, positional advantage)
\end{itemize}




\subsection*{b) A pick and place robot}

\paragraph*{States}
\begin{itemize}
    \item position and orientation of the axes
    \item is holding something
    \item source of objects and destination
\end{itemize}

\paragraph*{Actions}
\begin{itemize}
    \item pick an object
    \item place an object
    \item move
\end{itemize}


\paragraph*{Reward Signal}
\begin{itemize}
    \item successfully pick and place an object
    \item time to pick and place an object
    \item lost an object
\end{itemize}




\subsection*{c) A drone which should stabelize in air}

\paragraph*{States}
\begin{itemize}
    \item tilt angle
    \item velocity in the three axes
    \item angular velocity
    \item crashed
\end{itemize}

\paragraph*{Actions}
\begin{itemize}
    \item adapt speed of individual rotors
\end{itemize}

\paragraph*{Reward Signal}
\begin{itemize}
    \item time in air
    \item minimize the tilt angle
    \item minimize steering (and energy consumption)
\end{itemize}


\subsection*{d) Playing tetris}

\paragraph*{States}
\begin{itemize}
    \item position of the falling block
    \item position of the other blocks
    \item preview of next block
\end{itemize}


\paragraph*{Actions}
\begin{itemize}
    \item move block left/right
    \item rotate block
\end{itemize}

\paragraph*{Reward Signal}
\begin{itemize}
    \item clear a row
    \item lose the game
\end{itemize}



\section{Value Functions}

\paragraph*{a) k-armed Bandits as MDP}

The Future rewards are independent on the current state. Since there is only one state, i.e. you are about to roll a bandit, the only thing which could change is our policy based on new estimates of the true distribution - exploration. 

This is not considered in the expected reward and therefore its basicly  the same $R_t = R$ for all rolls. Then it holds that the sum is just an additional factor, which is not relevant for the value function
\begin{equation}
    \sum_{k=0}^\infty \gamma^k R_{t+k+1} = R  \sum_{k=0}^\infty \gamma^k = R \frac{1}{1-\gamma}
\end{equation} 
since $\gamma < 1$.


\paragraph*{b) Proof Relation of value and action-value function}

We shall show that 
\begin{equation}
    v_\pi(s) = \sum_a \pi(a|s) q_\pi(s,a)
\end{equation}
holds. We can use the definition of the value function
\begin{equation}
    v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\end{equation}
and the definition of the action-value function
\begin{equation}
    q_\pi(s,a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
\end{equation}
to show the relation. We use the law of total expectation to condition on the action $A_t = a$:
\begin{align}
    v_\pi(s) &= \mathbb{E}_\pi[G_t | S_t = s] \\
       &= \sum_a \pi(a|s) \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \cdot \\
       &= \sum_a \pi(a|s) q_\pi(s,a)
\end{align}


\paragraph*{c) Rephrase value function}

The bellman equation is given by
\begin{equation}
    v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma v_\pi(s')]
\end{equation}
We can execute the sum over $r$ and note that the reward is dependent on $r(s, a, s')$ and the transition probability is dependent on $p(s'|s,a)$, i.e.
\begin{align}
    \sum_{s',r} p(s',r|s,a) r &= \sum_{s'} p(s'|s,a) r(s, a, s')
\end{align}
Which yields the rephrased value function
\begin{equation}
    v_\pi(s) = \sum_a \pi(a|s) \sum_{s'} p(s'|s,a) [r(s, a, s') + \gamma v_\pi(s')]
\end{equation}



\section{Bruteforce the Policy Space}

\paragraph*{a) Number of policies}

There are $3\cdot 3 = 9$ tiles, which are the states. There are four actions, namely up, down, left and right. If the policy is deterministic, then for each tile the decision tree branches into four additional options, i.e.
\begin{equation}
    n_\pi = a^s = 4^{9} = 2^{18} = 262144
\end{equation}
which is do-able for a computer but still kind of ridiculos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{IceIceBaby.png}
    \caption{3x3 gridworld of icy lake}
    \label{fig:ice}
\end{figure}



\paragraph*{b) Do you expect the behaviour?}

That the left policy can only fail by starting on the left of the object is reasonable. I would have expected that tile $(x, y) = (1, 1)$ would have a higher value  
\begin{figure}[H]
    \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c}
    0 & 0 & 0.537 \\ \hline
    0 & 0 & 1.477 \\ \hline              
    0 & 0 & 5 \\
    \end{tabular}
    \caption{$v_\pi$ for $\pi =$Go left policy}
    \label{tab:table1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c}
    0.414 & 0.775 & 1.311 \\ \hline
    0.364 & 0.819 & 2.295 \\ \hline                 
    0.132 & 0 & 5 \\
    \end{tabular}
    \caption{$v_\pi$ for $\pi =$Go right policy}
    \label{tab:table2}
    \end{minipage}
    \end{figure}


\paragraph*{c) Bruteforce}

There are two optimal policies for the given optimal value function $v_*$, which are shown in Table \ref{tab:table3}. Namely, 

\begin{figure}[H]
\begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c}
    0.498 & 0.832 & 1.311 \\ \hline
    0.536 & 0.977 & 2.295 \\ \hline                 
    0.306 & 0 & 5 \\
    \end{tabular}
    \caption{Optimal value $v_*$}
    \label{tab:table3}
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c}
     1/2 & 2 & 2 \\ \hline
     3 & 3 & 2 \\ \hline
     0 & - & -
    \end{tabular}
    \caption{Best policies $\pi_*$}
    \label{tab:bestPol}
\end{minipage}    
\end{figure}






\paragraph*{d) Is Bruteforce a good idea?}

No. In general it is not feasable to Bruteforce any algorithm, which scales exponentially $O(e^n)$. For larger maps this is the case. The assumption for our solution is to calculate every policy and find the optimal one. Other algorithms should be used, which dont calculate the whole space. 

\paragraph*{NOTE}
All of the value functions should be devided by a fifth, because in terminal states it alows to go to terminal states again. Reworked the helper function \texttt{trans\_matrix\_for\_policy}.

\end{document}