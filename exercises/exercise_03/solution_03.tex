\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{bbm}
\title{Reinforcement Learning \\ Exercise 2 - Solution}
\author{Jonathan Schnitzler - st166934 \\
ErickVillanuevaVillasenor - st190300 \\
Eric Choquet - st160996}
\date{\today}
\begin{document}
\maketitle
\section*{Proofs}

\paragraph*{a) Bellman optimality operator is a gamma-contraction}
We want to show
\begin{equation}
    (\mathcal{T}v)(s) = \max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v(s')]
\end{equation}
fullfills the $\gamma$-contraction property, namely
\begin{equation}
    \|\mathcal{T}v - \mathcal{T}w\|_\infty \leq \gamma \|v - w\|_\infty
\end{equation}
Inspired by the lecture for the Bellman expectation backup operator, we will similarly use the definition of the infinity norm to show the contraction property
\begin{align}
    \|\mathcal{T}v - \mathcal{T}w\|_\infty &= \| \max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v(s')] - \max_a \sum_{s', r} p(s', r|s,a)[r + \gamma w(s')] \| \\
    &\leq \| \max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v(s') - (r + \gamma w(s'))] \| \\
    &=\gamma \| \max_a \sum_{s', r} p(s', r|s,a) [v(s') - w(s')] \| \\
    &\leq \gamma \| \max_a \sum_{s', r} p(s', r|s,a) \|v(s') - w(s')\|_\infty \| \\
    &\leq \gamma \|v - w\|_\infty
\end{align}
\end{document}