\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{bbm}
\usepackage{multirow}

\title{Reinforcement Learning \\ Exercise 6 - Solution}
\author{Jonathan Schnitzler - st166934 \\
Eric Choquet - st160996}
\date{\today}
\begin{document}
\maketitle

\section{Planning and Learning}

\paragraph*{a) Why did Dyna-Q+ perform better in both test phase}

Lets start with the \textbf{second phase}. For the phase of the suddenly appearing shortcut in the beginning of the wall, Dyna-Q+ is able to exploit the gained benefit faster, since it generates a higher reward since the state was not visited for a long time. On the other hand Dyna-Q remains on its fixed strategy which proofed to perform better for a longer period of time(i.e. the hole on the left of the wall) and therefore remains slow. \\

Lets consider the \textbf{first phase} and the tricky question: Why doesn't it cost Dyna-Q+ to explore its environment? This can be explained by living in a grid universe and one can not exploit walking diagonal. Therefore, each path to the hole in the wall is of the same length. Dyna-Q+ performs initially better, since it tends to explore the terrain and finds the hole in the wall earlier.


\paragraph*{b) Tabular Dyna-Q algorithm}

Adaptations in order to include stochastic environments could be achieved by implementing a stochastic process in the model itself. This can be done in multiple ways, I will present two approaches here. Either, probability is directly sampled by occurence
\begin{align}
    Model(S,A) &\leftarrow R_i, S'_i \quad \text{for} i = 1,...,N \\
    Model(S, A) &:= \begin{cases} R_1, \quad x < p_1 \\
                                  \qquad \vdots \\
                                  R_j, \quad x < \sum_{i=1}^j p_j \\
                                  \qquad \vdots \\
                                  R_n, \quad x < 1
                    \end{cases}
\end{align}
where $p_j = \frac{\# R_j}{N}$ and $x$ is a random number drawn from a uniform distribution over the interval $[0, 1]$. Alternatively one can use a Kernel-interpolation, e.g. with a gaussian Kernel from each Reward (and State if they are also continuos otherwise either do first method or floor to integer representation). 

Does this still perform well on changing environments? - If not how could it? The problem is that in the planning phase the $Q$ value for the cost of the state action, could be reduced to such an extend, that it doesn't visit the state again. This can again be solved by using a term in the reward to make state which haven't benn visited for a long time more attractive via Dyna-Q+.



\section{Monte Carlo Tree Search on the Taxi environment}

\paragraph{a) Performance of the MCTS algorithm}

In the accompanying scrupt \texttt{ex06\_plan.py} a monte carlo tree approach is implemented. As a test, the Taxi environment is used. It consist out of a yellow cab and four zones where the cab can pick up and drop off passengers. The cab can move in four directions (up, down, left, right) and can pick up and drop off passengers. The goal is to pick up a passenger at a certain location and drop him off at another location. The reward is given by the distance driven and the number of steps taken. The algorithm is tested with different values for the number of iterations \texttt{maxiter} and repeated three times for stability. The results are shown in figure \ref{fig:terminal}. 
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/terminal.png}
\caption{Output for Trees with \texttt{maxiter} = [10, 20, 50, 100, 200, 500]}
\label{fig:terminal}
\end{figure}
Another way to display this is via an table
\begin{table}[H]
\centering
\begin{tabular}{l|c|c|c|c}
\texttt{maxiter} & 1st Run & 2nd run & 3rd run & average  \\  \hline
 10& -605 &  -632 & -615 & -617 \\  \hline 
 20& -848 & -812 & -821 & -827  \\ \hline
 50& -839 & -320 & -767 & -642  \\ \hline
 100&-830& -666 & -704 & -733  \\ \hline
 200&-731&-839&-812&-794  \\ \hline
 500& -794&-767&-839 & -800
\end{tabular}
\caption{Tabular Version of terminal output}
\label{tab:t1}
\end{table}




% [7, 7, 6, 7, 6, 7, 7, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 7, 5, 7, 7, 6, 7, 7, 8, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 6, 8, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 6, 7, 7, 6, 6, 7, 8, 6, 7, 7, 7, 6, 6, 7, 6, 8, 7, 8, 8, 6, 6, 7, 7, 8, 7, 7, 5, 6, 8, 7, 7, 7, 6, 7, 8, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 7, 8, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 8, 7, 7, 6, 7, 7, 7, 7, 7, 7, 8, 7, 7, 6, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 6, 7, 7, 7, 6, 6, 7, 7, 6, 8, 6, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 6]

\paragraph*{b) Depth evaluation of trees}

To our surprise the tree doesn't seem to grow with the number of iterations. It stays roughly stable at a depth of 7. This is shown in figure \ref{fig:tree}. This is a bit surprising, since one would expect the tree to grow with the number of iterations. 
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/depth_evolution.png}
\caption{depth evaluation for Trees with \texttt{maxiter} = 500}
\label{fig:tree}
\end{figure}

\end{document}













