{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Summary\n",
    "\n",
    "Lecture summer 2024 by *Prof. Matthias Niepert* at university of Stuttgart\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Markov Decision Processes](#markov-decision-processes)\n",
    "3. [Monte Carlo](#monte-carlo)\n",
    "4. [Temporal Difference](#temporal-difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### RL - Problem\n",
    "* General framework for decision making\n",
    "* Agent -> max reward (long-term)\n",
    "\n",
    "<img src=\"slides/images/RL_Context.png\" width=\"40%\">\n",
    "<img src=\"slides/images/RL_Context_ML.png\" width=\"40%\">\n",
    "\n",
    "#### RL - Cycle\n",
    "<img src=\"slides/images/RL_Cycle.png\" width=\"80%\">\n",
    "\n",
    "with state $S_t$, action $A_t$, reward $R_t$ and history $H_t$\n",
    "\n",
    "$$ \\begin{align} H_t = S_0, A_0, R_1, S_1, A_1, R_2, ... S_{t-1}, A_{t-1}, R_t \\end{align} $$\n",
    "\n",
    "\n",
    "#### Markov Property\n",
    "\n",
    "A state $S_t$ is Markov if and only if\n",
    "$$ \\begin{align} \\text{Pr}\\{S_{t+1}\\} = \\text{Pr}\\{ S_{t+1} | S_1, ..., S_t \\} \\end{align} $$\n",
    "\n",
    "e.i. the future is independent of the past given the present.\n",
    "\n",
    "#### Markov - Types\n",
    "\n",
    "* Agent observes Markov state - Markov Decision Process (MDP)\n",
    "\n",
    "* observes indirectly $\\to$ Partially Obeservable MDP (POMDP) \n",
    "\n",
    "\n",
    "### RL - Agent\n",
    "\n",
    "* Policy $\\pi$ - agent's behavior (det. or stoch.)\n",
    "* Value function $V(s)$ - expected return from state $s$\n",
    "* Action-value function $Q(s,a)$ - expected return from state $s$ and action $a$\n",
    "* Model - agent's representation of the environment\n",
    "\n",
    "The true model aka transition function is given by\n",
    "$$ \\begin{align} p(s',r|s,a) = \\text{Pr}\\{ S_{t+1} = s', R_{t+1} = r| S_t = s, A_t = a\\} \\end{align} $$\n",
    "\n",
    "#### RL - Flavors\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((RL))\n",
    "    model-based\n",
    "      id[test]\n",
    "    model-free\n",
    "      value-based\n",
    "      policy-based\n",
    "      actor-critic\n",
    "    imitation learning\n",
    "```\n",
    "Exploitation vs Exploration actions, where the reward follows a probability distribution -> max. expected reward\n",
    "\n",
    "#### Bandits - Tabular solution methods\n",
    "\n",
    "If state and action spaces are small enough\n",
    "* find exact solution\n",
    "  * optimal V\n",
    "  * optimal $\\pi$\n",
    "\n",
    "Code example can be found [here](exercises/exercise_01/ex01-bandits.py)\n",
    "\n",
    "#### Greedy and Eplsilon-Greedy\n",
    "\n",
    "* Greedy for $\\epsilon = 0$ which is the percentage of doing an other (random or softmax) action instead of the believed best action\n",
    "\n",
    "$$ \\begin{align} A_t = \\text{argmax}_a Q_t(a) \\end{align} $$\n",
    "\n",
    "* Finetuning $\\varepsilon$\n",
    "  * reward variance is small, e.g. zero\n",
    "  * reward variance is large\n",
    "  * task is non-stationary\n",
    "\n",
    "$$ \\begin{align} \\pi_t(a) = \\frac{e^{\\frac{Q_t(a)}{\\tau}}}{\\sum_{a' = 1}^k e^{\\frac{Q_t(a')}{\\tau}}} \\end{align} $$\n",
    "\n",
    "Gibbs or Boltzmann distribution with Temperature $\\tau$ which is continuous $0$ and random $\\infty$.\n",
    "\n",
    "Incremental equation\n",
    "$$ \\begin{align} Q_{n+1} = Q_n + \\underbrace{\\frac{1}{n}}_{\\text{generally } \\alpha} [R_n - Q_n] \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "### Definitions\n",
    "\n",
    "<img src=\"slides/images/MDP_Defi.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Cumulative reward with discount factor\n",
    "\n",
    "$$ \\begin{align} G_t &= \\sum_{i=0}^\\infty \\gamma^i R_{t+i+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\end{align} $$\n",
    "\n",
    "with $\\gamma \\in [0,1]$\n",
    "\n",
    "#### Transition Function\n",
    "\n",
    "$$ \\begin{align} p(s'|s,a) &= \\text{Pr}\\{ S_{t+1} = s'|S_t=s, A_t = a \\} \\\\ &= \\sum_{r \\in \\mathcal{R}} p(s',r|s,a) \\end{align} $$\n",
    "\n",
    "#### Reward function\n",
    "\n",
    "immediate reward\n",
    "\n",
    "$$ \\begin{align} r(s,a,s') &= \\mathbb{E}\\left[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s'\\right] \\\\ &= \\sum_{r \\in \\mathcal{R}} r \\frac{p(s',r|s,a)}{p(s'|s,a)} \\end{align} $$\n",
    "\n",
    "Note - typically we assume there is a single reward for each $s,a,s'$ and drop the $\\mathbb{E}$.\n",
    "\n",
    "##### Collection rewards\n",
    "\n",
    "$$ \\begin{align} r(s,a) &= \\sum_{r \\in \\mathcal{R}} r \\sum_{s'\\in \\mathcal{S}} p(s',r|s,a) \\\\ r(s) &= ...\n",
    " \\end{align} $$\n",
    "\n",
    " #### Transition Graph\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    id0((low)) --> id01[recharge]\n",
    "    id01 --$$1,\\:0$$--> id1((high))\n",
    "\n",
    "\n",
    "    id0 --$$1, r_{wait}$$--> id02[search]\n",
    "    id02 --$$\\beta, r_{search}$$--> id0\n",
    "    id02 --$$1-\\beta, -3$$--> id1\n",
    "\n",
    "    id0 --> id03[wait]\n",
    "    id03 --$$1,\\: r_{wait}$$--> id0\n",
    "\n",
    "\n",
    "    id1 --> id11[search]\n",
    "    id11 --$$\\alpha, r_{search}$$--> id1\n",
    "    id11 --$$1-\\alpha, \\: r_{search}$$--> id0\n",
    "\n",
    "    id1 --> id12[wait]\n",
    "    id12 --$$1, r_{search}$$--> id1\n",
    "\n",
    "```\n",
    "\n",
    "#### Bellman Equation\n",
    "\n",
    "Recursive equation which is stationary for optimum\n",
    "\n",
    "##### Value Function\n",
    "$$ \\begin{align} v_\\pi(s) &= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_\\pi(s')\\right] \\qquad \\forall \\: s \\in \\mathcal{S} \\\\\n",
    "v_*(s) &= \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_*(s')\\right] \n",
    "\\end{align}$$\n",
    "##### Action-Value Function\n",
    "$$\\begin{align}\n",
    "q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s',a')\\right] \\\\\n",
    "q_*(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \n",
    "\\max_{a'} q_*(s',a')\\right] \\end{align}$$\n",
    "\n",
    "##### Relation toward each other\n",
    "$$ \\begin{align}\n",
    "q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_\\pi(s') \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "since $v_\\pi(s) = \\sum_{a} \\pi(a|s) q_\\pi(s,a)$\n",
    "\n",
    "\n",
    "#### Bellman: Matrix Form\n",
    "\n",
    "$$ \\begin{align} v = (I - \\gamma\\mathcal{P})^{-1}\\mathcal{R} \\end{align}$$\n",
    "\n",
    "where $\\mathcal{P}$ is the transition matrix \n",
    "$$ \\begin{align}\\mathcal{P} = \\begin{pmatrix} p_{11} & \\dots & p_{1n}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n1} & \\dots & p_{nn} \\end{pmatrix} \\end{align} $$\n",
    "and $\\mathcal{R}$ the reward matrix\n",
    "\n",
    "\n",
    "#### Optimal Policy\n",
    "\n",
    "$$ \\begin{align} v_*(s) &= \\max_\\pi v_\\pi(s) \\quad \\forall \\: s \\in \\mathcal{S} \\\\\n",
    "q_*(s,a) &= \\max_\\pi \\underbrace{\\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = t \\right]}_{q_\\pi(s,a)} \\end{align} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current environment: \n",
      "\n",
      "\u001b[41mS\u001b[0mFF\n",
      "FFF\n",
      "FHG\n",
      "\n",
      "Value function for policy_left (always going left):\n",
      "[0.         0.         0.10738255 0.         0.         0.29530201\n",
      " 0.         0.         1.        ]\n",
      "Value function for policy_right (always going right):\n",
      "[0.08280355 0.15491253 0.26229508 0.07279724 0.16371438 0.45901639\n",
      " 0.02647172 0.         1.        ]\n",
      "Optimal value function:\n",
      "[0.09951342 0.16642762 0.26229508 0.10723429 0.19538088 0.45901639\n",
      " 0.06127674 0.         1.        ]\n",
      "number optimal policies:\n",
      "2\n",
      "optimal policies:\n",
      "[[1 2 2 3 3 2 0 1 1]\n",
      " [2 2 2 3 3 2 0 1 1]]\n",
      "rollout policy:\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mF\n",
      "FFF\n",
      "FHG\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 142\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 131\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxiter):\n\u001b[0;32m--> 131\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[43moptimalpolicies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_state \u001b[38;5;129;01min\u001b[39;00m terminals():\n\u001b[1;32m    133\u001b[0m         terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Bellman Equation solved for slippery gridworld\n",
    "import numpy as np\n",
    "import gym\n",
    "from itertools import product\n",
    "\n",
    "custom_map3x3 = [\n",
    "    'SFF',\n",
    "    'FFF',\n",
    "    'FHG',\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v0\", desc=custom_map3x3)\n",
    "\n",
    "\n",
    "# Init some useful variables:\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "r = np.zeros(n_states) # the r vector is zero everywhere except for the goal state (last state)\n",
    "r[-1] = 1.\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "\n",
    "\"\"\" This is a helper function that returns the transition probability matrix P for a policy \"\"\"\n",
    "def trans_matrix_for_policy(policy):\n",
    "    s_term = terminals()\n",
    "    possible_states = [s for s in range(n_states) if s not in s_term]\n",
    "    transitions = np.zeros((n_states, n_states))\n",
    "    for s in possible_states:\n",
    "        probs = env.P[s][policy[s]]\n",
    "        for el in probs:\n",
    "            transitions[s, el[1]] += el[0]\n",
    "    return transitions\n",
    "\n",
    "\n",
    "\"\"\" This is a helper function that returns terminal states \"\"\"\n",
    "def terminals():\n",
    "    terms = []\n",
    "    for s in range(n_states):\n",
    "        # terminal is when we end with probability 1 in terminal:\n",
    "        if env.P[s][0][0][0] == 1.0 and env.P[s][0][0][3] == True:\n",
    "            terms.append(s)\n",
    "    return terms\n",
    "\n",
    "\n",
    "def value_policy(policy):\n",
    "    P = trans_matrix_for_policy(policy)\n",
    "    P[-1] = 0\n",
    "    # TODO: calculate and return v\n",
    "    # (P, r and gamma already given)\n",
    "    v = np.linalg.solve(np.eye(n_states) - gamma * P, r)\n",
    "    return v\n",
    "\n",
    "\n",
    "def bruteforce_policies():\n",
    "    terms = terminals()\n",
    "    number_of_non_terminal_states = n_states - len(terms)\n",
    "    optimalpolicies = []\n",
    "    all_policies = []\n",
    "    number_of_policies = n_actions**number_of_non_terminal_states\n",
    "\n",
    "    for i in range(number_of_policies):\n",
    "        # Initialize an empty list to store the current policy\n",
    "        policy = []\n",
    "        # For each state, determine the corresponding action\n",
    "        # We do this by dividing by the appropriate power of n_actions\n",
    "        # and taking the modulus with n_actions\n",
    "        value = i\n",
    "        for j in range(number_of_non_terminal_states):\n",
    "            # Find the action for this state\n",
    "            action = value % n_actions  # Get the remainder to determine the action\n",
    "            policy.append(action)\n",
    "\n",
    "            # Update the value to get the next action\n",
    "            value = value // n_actions\n",
    "\n",
    "        all_policies.append(policy)\n",
    "\n",
    "    # fill spots of terminal states with random action (it doesnt matter)\n",
    "    for term in terms:\n",
    "        for policy in all_policies:\n",
    "            policy.insert(term, 1)\n",
    "\n",
    "    all_policies = np.array(all_policies)\n",
    "\n",
    "    values = np.array([value_policy(policy) for policy in all_policies])\n",
    "    values_sum = values.sum(axis=1)\n",
    "    optimalvalue_index = np.argmax(values_sum)\n",
    "    optimalvalue = values[optimalvalue_index]\n",
    "\n",
    "    matches = [np.array_equal(optimalvalue, sub_array) for sub_array in values]\n",
    "    optimalpolicies = all_policies[matches]\n",
    "    \n",
    "\n",
    "    print(\"Optimal value function:\")\n",
    "    print(optimalvalue)\n",
    "    print(\"number optimal policies:\")\n",
    "    print(len(optimalpolicies))\n",
    "    print(\"optimal policies:\")\n",
    "    print(np.array(optimalpolicies))\n",
    "\n",
    "    return optimalpolicies\n",
    "\n",
    "\n",
    "def main():\n",
    "    # print the environment\n",
    "    print(\"current environment: \")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    print(\"\")\n",
    "\n",
    "    # Here a policy is just an array with the action for a state as element\n",
    "    policy_left = np.zeros(n_states, dtype=int)  # 0 for all states\n",
    "    policy_right = np.ones(n_states, dtype=int) * 2  # 2 for all states\n",
    "\n",
    "    # Value functions:\n",
    "    print(\"Value function for policy_left (always going left):\")\n",
    "    print(value_policy(policy_left))\n",
    "    print(\"Value function for policy_right (always going right):\")\n",
    "    print(value_policy(policy_right))\n",
    "\n",
    "    optimalpolicies = bruteforce_policies()\n",
    "\n",
    "\n",
    "    # This code can be used to \"rollout\" a policy in the environment:\n",
    "    print(\"rollout policy:\")\n",
    "    maxiter = 100\n",
    "    terminated = False\n",
    "    for i in range(10):\n",
    "        state, _ = env.reset()\n",
    "        for j in range(maxiter):\n",
    "            \n",
    "            new_state = env.step(optimalpolicies[0][state])\n",
    "            if new_state in terminals():\n",
    "                terminated = True\n",
    "            env.render()\n",
    "            state=new_state\n",
    "            if terminated:\n",
    "                print(\"Finished episode\")\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "\n",
    "```python\n",
    "for s in states:\n",
    "    v_temp = V[s]\n",
    "    V[s] = np.sum([pi[s,a] * np.sum([p[s,a,s1] * (r[s,a,s1] + gamma * V[s1]) for s1 in states]) for a in actions])\n",
    "    delta = max(delta, np.abs(v_temp - V[s]))\n",
    "```\n",
    "until $\\Delta < \\theta$.\n",
    "\n",
    "<img src=\"notes/image/algorithms/policy_iteration.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contraction\n",
    "\n",
    "Using the infinity norm, we can use the bellman equation as a contraction\n",
    "\n",
    "$$ \\begin{align} (T^\\pi v)(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v(s') \\right] \\end{align} $$\n",
    "\n",
    "proof involves property of maximum norm ($\\infty$-norm) see [here](slides/Lecture%2003.pdf)\n",
    "\n",
    "\n",
    "#### Asynchronous DP\n",
    "\n",
    "* In-place\n",
    "* Prioritized sweeping (e.g. Bellman error $|v_{bel} - v|) e.g. via queue\n",
    "* real-time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "\n",
    "Estimation of black box $f$ w.r.t. some distribution which satisfies\n",
    "\n",
    "* Samples i.i.d.\n",
    "* $f$ encodes environment and returns sequence of experience\n",
    "\n",
    "### MC in RL\n",
    "\n",
    "* **Learn** value function $v()$ from *experience* (env or sim)\n",
    "* **Discover** optimal policies\n",
    "* **Blackbox view** ($p(s'|s,a)$ and $r(s,a,s')$ isnt required)\n",
    "\n",
    "<img src=\"notes/image/blackbox.png\" width=\"50%\">\n",
    "\n",
    "#### MC principle\n",
    "\n",
    "* Divide into (terminating!) episodes\n",
    "* estimate $v(\\cdot)$ and update each **episode** (instead of each step like DP)\n",
    "* TODO: cant use $v(\\cdot)$ to improve $\\pi$\n",
    "* $v(\\cdot)$ is approx. for $s, \\pi$ pair\n",
    "    * first visit\n",
    "    * every visit\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC properties\n",
    "\n",
    "* Independent estimate for $v(\\cdot)$\n",
    "* Compute time is independent of $|\\mathcal{S}|$\n",
    "* Possible to focus on relevant states and ignore the value of others\n",
    "\n",
    "Need to estimate \n",
    "$$ \\begin{align} \\pi'(s) = \\argmax_a q_\\pi(s,a) \\end{align} $$\n",
    "\n",
    "And similar\n",
    "$$ \\begin{align} q_\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t|S_t = s, A_t = a \\right] \\end{align} $$\n",
    "by averaging returns following first visit to that state-action pair [Warning p.20](slides/Lecture%2004.pdf)\n",
    "\n",
    "#### MC control\n",
    "\n",
    "<img src=\"notes/image/MC_control.png\" width=\"50%\">\n",
    "\n",
    "* converges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### On-policy\n",
    "* learn about policy currently used to generate experience\n",
    "\n",
    "<img src=\"notes/image/algorithms/MC/First-visit_MC_control.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "### Off-policy\n",
    "* evaluate and improve a policy that is different from the one used for genereating episodes\n",
    "* *target* policy $\\pi$ and *behavior* policy $\\mu$\n",
    "* it holds that $\\mu$ generates behavior which covers $\\pi$ \n",
    "$$ \\begin{align} \\pi(a|s) > 0 \\implies \\mu(a|s) > 0 \\: \\forall_{s,a} \\end{align} $$\n",
    "*compute MC estimates from the trajectories generated by $\\mu$ but make adjustments such that we obtain estimates compatible with $\\pi$*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "* Target distribution $p(x)$\n",
    "* proposal distribution $q(x)$\n",
    "\n",
    "$$ \\begin{align} \\mathbb{E}_{p(x)}[f(x)] &= \\int f(x) p(x) \\mathrm{d}x = \\int f(x) \\frac{p(x)}{q(x)} q(x) \\mathrm{d}x = \\mathbb{E}_{q(x)} \\left[ f(x) \\frac{p(x)}{q(x)} \\right] \\\\\n",
    "&\\approx \\frac{1}{L} \\sum_{l=1}^L f(x_l) \\underbrace{\\frac{p(x_l)}{q(x_l)}}_{w_l}, \\quad \\text{with samples} \\: x_l \\overset{i.i.d.}{\\sim} q(x) \\end{align} $$\n",
    "\n",
    "See further [here](slides/Lecture%2004.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "* combines **sampling** [MC] with **bootstrapping** (update based in part on existing estimate) [DP] see page 9 [here](slides/Lecture%2005.pdf)\n",
    "\n",
    "### Incremental Monte Carlo\n",
    "\n",
    "$$ \\begin{align} V(s) \\leftarrow V(s) + \\underbrace{\\frac{1}{n(s) + 1}}_{\\alpha} [\\underbrace{G_t}_{\\text{MC target}} - V(s)] \\end{align} $$ \n",
    "where $\\alpha$ could be given as a relation with respect to the number of first visits of states $s$ \n",
    "\n",
    "Simplest temporal difference update TD(0)\n",
    "$$ target = R_{t+1} + \\gamma V(S_{t+1})$$\n",
    "\n",
    "\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| TD | MC |\n",
    "| --- | --- |\n",
    "| learn before terminal | unbiased |\n",
    "| non-episodic or episodic task | - |\n",
    "| lower variance | - |\n",
    "| bias - TD updates exploit the Markov property| |\n",
    "\n",
    "both converge but these value function are potentially different. See example [here](slides/Lecture%2005.pdf)\n",
    "\n",
    "<img src=\"notes/image/comparison_square.png\" width=\"60%\">\n",
    "<!-- <img src=\"notes/image/TD_MC_DP.png\" width=\"40%\"> -->\n",
    "\n",
    "### TD for control\n",
    "estimate $Q$ instead of $V$\n",
    "\n",
    "$$ Q(S_t, A+t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}) - Q(S_t, A_t)] $$\n",
    "\n",
    "* change $\\pi$ toward greediness wrt $Q_\\pi$\n",
    "* use soft policies, e.g. $\\varepsilon$-greedy\n",
    "* converges to $Q_*$\n",
    "\n",
    "#### SARSA (on-policy)\n",
    "<img src=\"notes/image/algorithms/SARSA.png\" width=\"80%\">\n",
    "\n",
    "#### Q-learning (off-policy)\n",
    "* Target policy $\\pi$ is greedy $$\\pi(a) = \\argmax_a Q(s,a)$$\n",
    "* Behavior policy $\\mu$ is soft plicy, e.g. $\\varepsilon-$ greedy\n",
    "<img src=\"notes/image/algorithms/Qlearning.png\" width=\"80%\">\n",
    "\n",
    "#### Comparison of Q-learning and SARSA\n",
    "\n",
    "<img src=\"notes/image/Sarsa_vs_Q.png\" width=\"80%\">\n",
    "\n",
    "#### Extensions\n",
    "\n",
    "##### Expected SARSA\n",
    "\n",
    "Both on- and off-policy\n",
    "\n",
    "\n",
    "##### Maximization bias\n",
    "\n",
    "* positive maximization bias\n",
    "\n",
    "<img src=\"notes/image/examples/max_bias.png\" width=\"30%\">\n",
    "\n",
    "##### Double Q-learning\n",
    "\n",
    "* Divide into $Q_1$ and $Q_2$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & Planning\n",
    "\n",
    "### Model\n",
    "* Anything the agent can use like an environment\n",
    "    * Distribution model (ideal)\n",
    "    * Sample model (approx.) \n",
    "\n",
    "### Planning\n",
    "* Uses model, e.g. $\\text{model} \\overset{\\text{planning}}{\\to}\\text{policy}$\n",
    "* Types\n",
    "  * state-space planning\n",
    "    * search through the state space\n",
    "    * e.g. classical DP, heustic search\n",
    "  * plan-space (IGNORE, not done in lecture)\n",
    "\n",
    "\n",
    "<img src=\"notes/image/RL_cycle.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "| Direct | Indirect |\n",
    "| --- | --- |\n",
    "| simpler | fuller use of experience |\n",
    "| not affected by bad model | better policy with fewer env it |\n",
    "\n",
    "* can occur *simultaneously* and in *parallel*\n",
    "\n",
    "\n",
    "\n",
    "### Dyna architecture\n",
    "\n",
    "<img src=\"notes/image/algorithms/DynaQ.png\" width=\"80%\">\n",
    "\n",
    "#### DYna-Q+\n",
    "\n",
    "Uses exploration bonus\n",
    "\n",
    "$$ \\begin{align} R + \\kappa \\sqrt{\\tau} \\end{align} $$\n",
    "\n",
    "\n",
    "### Prioritized sweeping\n",
    "\n",
    "* which $s$ or $(s,a)$ is nice in planning\n",
    "  * maintain a queue $(s,a)$ who change a lot\n",
    "  * when a new backup occurs insert predecessors according to their priorities\n",
    "  * always perform backups from first in queue\n",
    "\n",
    "  <img src=\"notes/image/algorithms/PrioritizedSweeping.png\" width=\"80%\">\n",
    "  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation based search\n",
    "* Planning after encountering a new state $S_t$ (instead of imroving overall policy)\n",
    "\n",
    "#### Forward search\n",
    "* build a search tree with the current state $s_t$\n",
    "\n",
    "<img src=\"notes/image/forward_search.png\" width=\"50%\">\n",
    "\n",
    "#### Rollout\n",
    "\n",
    "TODO: simulate from this point onwards, the model therefore is\n",
    "\n",
    "$$ \\begin{align} \\{s_t^k, A_t^k, R_{t+1}^k,\\dots, S_T^k\\}_{k=1}^K \\sim \\mathcal{M}_v \\end{align} $$\n",
    "\n",
    "Then we can apply model-free RL to simulate episode, e.g\n",
    "* MC control $\\to$ [MC search](#mc-search)\n",
    "* SARSA $\\to$ TD search\n",
    "\n",
    "#### MC search\n",
    "\n",
    "##### Simple\n",
    "\n",
    "* Model $\\mathcal{M}_v$, policy $\\pi$\n",
    "* For each action $a \\in \\mathcal{A}$\n",
    "$$ \\begin{align} \\{s_t^k, a_t, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k \\dots, S_T^k\\}_{k=1}^K \\sim \\mathcal{M}_v \\end{align} $$\n",
    "\n",
    "evaluate by mean return\n",
    "\n",
    "$$ \\begin{align} Q(s_t, a_t) = \\frac{1}{N} \\sum_{k=1}^K G_t \\overset{P}{\\to} q_\\pi(s_t, a)  \\end{align} $$\n",
    "\n",
    "$$ a_t = \\argmax_{a \\in \\mathcal{A}} Q(s_t, a)$$\n",
    "\n",
    "\n",
    "##### Tree\n",
    "\n",
    "* Selection\n",
    "* Expansion\n",
    "* Simulation\n",
    "  * Tree policy (improves)\n",
    "  * Rollout policy (fixed)\n",
    "* Backup\n",
    "\n",
    "\n",
    "<img src=\"notes/image/MC_tree_search.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
