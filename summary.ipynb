{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Summary\n",
    "\n",
    "Lecture summer 2024 by *Prof. Matthias Niepert* at university of Stuttgart\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Markov Decision Processes](#markov-decision-processes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### RL - Problem\n",
    "* General framework for decision making\n",
    "* Agent -> max reward (long-term)\n",
    "\n",
    "<img src=\"slides/images/RL_Context.png\" width=\"40%\">\n",
    "<img src=\"slides/images/RL_Context_ML.png\" width=\"40%\">\n",
    "\n",
    "#### RL - Cycle\n",
    "<img src=\"slides/images/RL_Cycle.png\" width=\"80%\">\n",
    "\n",
    "with state $S_t$, action $A_t$, reward $R_t$ and history $H_t$\n",
    "\n",
    "$$ \\begin{align} H_t = S_0, A_0, R_1, S_1, A_1, R_2, ... S_{t-1}, A_{t-1}, R_t \\end{align} $$\n",
    "\n",
    "\n",
    "#### Markov Property\n",
    "\n",
    "A state $S_t$ is Markov if and only if\n",
    "$$ \\begin{align} \\text{Pr}\\{S_{t+1}\\} = \\text{Pr}\\{ S_{t+1} | S_1, ..., S_t \\} \\end{align} $$\n",
    "\n",
    "e.i. the future is independent of the past given the present.\n",
    "\n",
    "#### Markov - Types\n",
    "\n",
    "* Agent observes Markov state - Markov Decision Process (MDP)\n",
    "\n",
    "* observes indirectly $\\to$ Partially Obeservable MDP (POMDP) \n",
    "\n",
    "\n",
    "### RL - Agent\n",
    "\n",
    "* Policy $\\pi$ - agent's behavior (det. or stoch.)\n",
    "* Value function $V(s)$ - expected return from state $s$\n",
    "* Action-value function $Q(s,a)$ - expected return from state $s$ and action $a$\n",
    "* Model - agent's representation of the environment\n",
    "\n",
    "The true model aka transition function is given by\n",
    "$$ \\begin{align} p(s',r|s,a) = \\text{Pr}\\{ S_{t+1} = s', R_{t+1} = r| S_t = s, A_t = a\\} \\end{align} $$\n",
    "\n",
    "#### RL - Flavors\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((RL))\n",
    "    model-based\n",
    "      id[test]\n",
    "    model-free\n",
    "      value-based\n",
    "      policy-based\n",
    "      actor-critic\n",
    "    imitation learning\n",
    "```\n",
    "Exploitation vs Exploration actions, where the reward follows a probability distribution -> max. expected reward\n",
    "\n",
    "#### Bandits - Tabular solution methods\n",
    "\n",
    "If state and action spaces are small enough\n",
    "* find exact solution\n",
    "  * optimal V\n",
    "  * optimal $\\pi$\n",
    "\n",
    "Code example can be found [here](exercises/exercise_01/ex01-bandits.py)\n",
    "\n",
    "#### Greedy and Eplsilon-Greedy\n",
    "\n",
    "* Greedy for $\\epsilon = 0$ which is the percentage of doing an other (random or softmax) action instead of the believed best action\n",
    "\n",
    "$$ \\begin{align} A_t = \\text{argmax}_a Q_t(a) \\end{align} $$\n",
    "\n",
    "* Finetuning $\\varepsilon$\n",
    "  * reward variance is small, e.g. zero\n",
    "  * reward variance is large\n",
    "  * task is non-stationary\n",
    "\n",
    "$$ \\begin{align} \\pi_t(a) = \\frac{e^{\\frac{Q_t(a)}{\\tau}}}{\\sum_{a' = 1}^k e^{\\frac{Q_t(a')}{\\tau}}} \\end{align} $$\n",
    "\n",
    "Gibbs or Boltzmann distribution with Temperature $\\tau$ which is continuous $0$ and random $\\infty$.\n",
    "\n",
    "Incremental equation\n",
    "$$ \\begin{align} Q_{n+1} = Q_n + \\underbrace{\\frac{1}{n}}_{\\text{generally } \\alpha} [R_n - Q_n] \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "### Definitions\n",
    "\n",
    "<img src=\"slides/images/MDP_Defi.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Cumulative reward with discount factor\n",
    "\n",
    "$$ \\begin{align} G_t &= \\sum_{i=0}^\\infty \\gamma^i R_{t+i+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\end{align} $$\n",
    "\n",
    "with $\\gamma \\in [0,1]$\n",
    "\n",
    "#### Transition Function\n",
    "\n",
    "$$ \\begin{align} p(s'|s,a) &= \\text{Pr}\\{ S_{t+1} = s'|S_t=s, A_t = a \\} \\\\ &= \\sum_{r \\in \\mathcal{R}} p(s',r|s,a) \\end{align} $$\n",
    "\n",
    "#### Reward function\n",
    "\n",
    "immediate reward\n",
    "\n",
    "$$ \\begin{align} r(s,a,s') &= \\mathbb{E}\\left[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s'\\right] \\\\ &= \\sum_{r \\in \\mathcal{R}} r \\frac{p(s',r|s,a)}{p(s'|s,a)} \\end{align} $$\n",
    "\n",
    "Note - typically we assume there is a single reward for each $s,a,s'$ and drop the $\\mathbb{E}$.\n",
    "\n",
    "##### Collection rewards\n",
    "\n",
    "$$ \\begin{align} r(s,a) &= \\sum_{r \\in \\mathcal{R}} r \\sum_{s'\\in \\mathcal{S}} p(s',r|s,a) \\\\ r(s) &= ...\n",
    " \\end{align} $$\n",
    "\n",
    " #### Transition Graph\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    id0((low)) --> id01[recharge]\n",
    "    id01 --$$1,\\:0$$--> id1((high))\n",
    "\n",
    "\n",
    "    id0 --$$1, r_{wait}$$--> id02[search]\n",
    "    id02 --$$\\beta, r_{search}$$--> id0\n",
    "    id02 --$$1-\\beta, -3$$--> id1\n",
    "\n",
    "    id0 --> id03[wait]\n",
    "    id03 --$$1,\\: r_{wait}$$--> id0\n",
    "\n",
    "\n",
    "    id1 --> id11[search]\n",
    "    id11 --$$\\alpha, r_{search}$$--> id1\n",
    "    id11 --$$1-\\alpha, \\: r_{search}$$--> id0\n",
    "\n",
    "    id1 --> id12[wait]\n",
    "    id12 --$$1, r_{search}$$--> id1\n",
    "\n",
    "```\n",
    "\n",
    "#### Bellman Equation\n",
    "\n",
    "Recursive equation which is stationary for optimum\n",
    "\n",
    "##### Value Function\n",
    "$$ \\begin{align} v_\\pi(s) &= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_\\pi(s')\\right] \\qquad \\forall \\: s \\in \\mathcal{S} \\\\\n",
    "v_*(s) &= \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_*(s')\\right] \n",
    "\\end{align}$$\n",
    "##### Action-Value Function\n",
    "$$\\begin{align}\n",
    "q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s',a')\\right] \\\\\n",
    "q_*(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \n",
    "\\max_{a'} q_*(s',a')\\right] \\end{align}$$\n",
    "\n",
    "##### Relation toward each other\n",
    "$$ \\begin{align}\n",
    "q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_\\pi(s') \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "since $v_\\pi(s) = \\sum_{a} \\pi(a|s) q_\\pi(s,a)$\n",
    "\n",
    "\n",
    "#### Bellman: Matrix Form\n",
    "\n",
    "$$ \\begin{align} v = (I - \\gamma\\mathcal{P})^{-1}\\mathcal{R} \\end{align}$$\n",
    "\n",
    "where $\\mathcal{P}$ is the transition matrix \n",
    "$$ \\begin{align}\\mathcal{P} = \\begin{pmatrix} p_{11} & \\dots & p_{1n}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n1} & \\dots & p_{nn} \\end{pmatrix} \\end{align} $$\n",
    "and $\\mathcal{R}$ the reward matrix\n",
    "\n",
    "\n",
    "#### Optimal Policy\n",
    "\n",
    "$$ \\begin{align} v_*(s) &= \\max_\\pi v_\\pi(s) \\quad \\forall \\: s \\in \\mathcal{S} \\\\\n",
    "q_*(s,a) &= \\max_\\pi \\underbrace{\\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = t \\right]}_{q_\\pi(s,a)} \\end{align} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellman Equation solved for slippery gridworld\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def value_iteration(env, gamma=0.9, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        q = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q[a] = sum([p * (r + gamma * V[s_]) for p, s_, r, _ in env.P[s][a]])\n",
    "        policy[s, np.argmax(q)] = 1\n",
    "    return policy, V\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "policy, V = value_iteration(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 16 into shape (16,16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     ax\u001b[38;5;241m.\u001b[39madd_table(tb)\n\u001b[1;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 26\u001b[0m plot_policy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 16 into shape (16,16)"
     ]
    }
   ],
   "source": [
    "# Visualize the grid and the policy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table\n",
    "\n",
    "def plot_policy(policy):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = policy.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(policy):\n",
    "        tb.add_cell(i, j, width, height, text=val, loc='center', facecolor='white')\n",
    "\n",
    "    # Row and column labels...\n",
    "    for i in range(len(policy)):\n",
    "        tb.add_cell(i, -1, width, height, text=i, loc='right', edgecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i, loc='center', edgecolor='none')\n",
    "\n",
    "    ax.add_table(tb)\n",
    "    plt.show()\n",
    "\n",
    "plot_policy(np.argmax(policy, axis=1).reshape(env.observation_space.n, env.observation_space.n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
