{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Summary\n",
    "\n",
    "Lecture summer 2024 by *Prof. Matthias Niepert* at university of Stuttgart\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Markov Decision Processes](#markov-decision-processes)\n",
    "3. [Monte Carlo](#monte-carlo)\n",
    "4. [Temporal Difference](#temporal-difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### RL - Problem\n",
    "* General framework for decision making\n",
    "* Agent -> max reward (long-term)\n",
    "\n",
    "<img src=\"slides/images/RL_Context.png\" width=\"40%\">\n",
    "<img src=\"slides/images/RL_Context_ML.png\" width=\"40%\">\n",
    "\n",
    "#### RL - Cycle\n",
    "<img src=\"slides/images/RL_Cycle.png\" width=\"80%\">\n",
    "\n",
    "with state $S_t$, action $A_t$, reward $R_t$ and history $H_t$\n",
    "\n",
    "$$ \\begin{align} H_t = S_0, A_0, R_1, S_1, A_1, R_2, ... S_{t-1}, A_{t-1}, R_t \\end{align} $$\n",
    "\n",
    "\n",
    "#### Markov Property\n",
    "\n",
    "A state $S_t$ is Markov if and only if\n",
    "$$ \\begin{align} \\text{Pr}\\{S_{t+1}\\} = \\text{Pr}\\{ S_{t+1} | S_1, ..., S_t \\} \\end{align} $$\n",
    "\n",
    "e.i. the future is independent of the past given the present.\n",
    "\n",
    "#### Markov - Types\n",
    "\n",
    "* Agent observes Markov state - Markov Decision Process (MDP)\n",
    "\n",
    "* observes indirectly $\\to$ Partially Obeservable MDP (POMDP) \n",
    "\n",
    "\n",
    "### RL - Agent\n",
    "\n",
    "* Policy $\\pi$ - agent's behavior (det. or stoch.)\n",
    "* Value function $V(s)$ - expected return from state $s$\n",
    "* Action-value function $Q(s,a)$ - expected return from state $s$ and action $a$\n",
    "* Model - agent's representation of the environment\n",
    "\n",
    "The true model aka transition function is given by\n",
    "$$ \\begin{align} p(s',r|s,a) = \\text{Pr}\\{ S_{t+1} = s', R_{t+1} = r| S_t = s, A_t = a\\} \\end{align} $$\n",
    "\n",
    "#### RL - Flavors\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((RL))\n",
    "    model-based\n",
    "      id[test]\n",
    "    model-free\n",
    "      value-based\n",
    "      policy-based\n",
    "      actor-critic\n",
    "    imitation learning\n",
    "```\n",
    "Exploitation vs Exploration actions, where the reward follows a probability distribution -> max. expected reward\n",
    "\n",
    "#### Bandits - Tabular solution methods\n",
    "\n",
    "If state and action spaces are small enough\n",
    "* find exact solution\n",
    "  * optimal V\n",
    "  * optimal $\\pi$\n",
    "\n",
    "Code example can be found [here](exercises/exercise_01/ex01-bandits.py)\n",
    "\n",
    "#### Greedy and Eplsilon-Greedy\n",
    "\n",
    "* Greedy for $\\epsilon = 0$ which is the percentage of doing an other (random or softmax) action instead of the believed best action\n",
    "\n",
    "$$ \\begin{align} A_t = \\text{argmax}_a Q_t(a) \\end{align} $$\n",
    "\n",
    "* Finetuning $\\varepsilon$\n",
    "  * reward variance is small, e.g. zero\n",
    "  * reward variance is large\n",
    "  * task is non-stationary\n",
    "\n",
    "$$ \\begin{align} \\pi_t(a) = \\frac{e^{\\frac{Q_t(a)}{\\tau}}}{\\sum_{a' = 1}^k e^{\\frac{Q_t(a')}{\\tau}}} \\end{align} $$\n",
    "\n",
    "Gibbs or Boltzmann distribution with Temperature $\\tau$ which is continuous $0$ and random $\\infty$.\n",
    "\n",
    "Incremental equation\n",
    "$$ \\begin{align} Q_{n+1} = Q_n + \\underbrace{\\frac{1}{n}}_{\\text{generally } \\alpha} [R_n - Q_n] \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "### Definitions\n",
    "\n",
    "<img src=\"slides/images/MDP_Defi.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Cumulative reward with discount factor\n",
    "\n",
    "$$ \\begin{align} G_t &= \\sum_{i=0}^\\infty \\gamma^i R_{t+i+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\end{align} $$\n",
    "\n",
    "with $\\gamma \\in [0,1]$\n",
    "\n",
    "#### Transition Function\n",
    "\n",
    "$$ \\begin{align} p(s'|s,a) &= \\text{Pr}\\{ S_{t+1} = s'|S_t=s, A_t = a \\} \\\\ &= \\sum_{r \\in \\mathcal{R}} p(s',r|s,a) \\end{align} $$\n",
    "\n",
    "#### Reward function\n",
    "\n",
    "immediate reward\n",
    "\n",
    "$$ \\begin{align} r(s,a,s') &= \\mathbb{E}\\left[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s'\\right] \\\\ &= \\sum_{r \\in \\mathcal{R}} r \\frac{p(s',r|s,a)}{p(s'|s,a)} \\end{align} $$\n",
    "\n",
    "Note - typically we assume there is a single reward for each $s,a,s'$ and drop the $\\mathbb{E}$.\n",
    "\n",
    "##### Collection rewards\n",
    "\n",
    "$$ \\begin{align} r(s,a) &= \\sum_{r \\in \\mathcal{R}} r \\sum_{s'\\in \\mathcal{S}} p(s',r|s,a) \\\\ r(s) &= ...\n",
    " \\end{align} $$\n",
    "\n",
    " #### Transition Graph\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    id0((low)) --> id01[recharge]\n",
    "    id01 --$$1,\\:0$$--> id1((high))\n",
    "\n",
    "\n",
    "    id0 --$$1, r_{wait}$$--> id02[search]\n",
    "    id02 --$$\\beta, r_{search}$$--> id0\n",
    "    id02 --$$1-\\beta, -3$$--> id1\n",
    "\n",
    "    id0 --> id03[wait]\n",
    "    id03 --$$1,\\: r_{wait}$$--> id0\n",
    "\n",
    "\n",
    "    id1 --> id11[search]\n",
    "    id11 --$$\\alpha, r_{search}$$--> id1\n",
    "    id11 --$$1-\\alpha, \\: r_{search}$$--> id0\n",
    "\n",
    "    id1 --> id12[wait]\n",
    "    id12 --$$1, r_{search}$$--> id1\n",
    "\n",
    "```\n",
    "\n",
    "#### Bellman Equation\n",
    "\n",
    "$$ \\begin{align} v_\\pi(s) &= \\mathbb{E}_\\pi [G_t | S_t = s] \\\\ \n",
    "q_\\pi(s,a) &= \\mathbb{E}_\\pi [G_t | S_t = s, A_t = a] \\end{align} $$\n",
    "\n",
    "Recursive equation which is stationary for optimum\n",
    "\n",
    "##### Value Function\n",
    "$$ \\begin{align} v_\\pi(s) &= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_\\pi(s')\\right] \\qquad \\forall \\: s \\in \\mathcal{S} \\\\\n",
    "v_*(s) &= \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_*(s')\\right] \n",
    "\\end{align}$$\n",
    "##### Action-Value Function\n",
    "$$\\begin{align}\n",
    "q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s',a')\\right] \\\\\n",
    "q_*(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \n",
    "\\max_{a'} q_*(s',a')\\right] \\end{align}$$\n",
    "\n",
    "##### Relation toward each other\n",
    "$$ \\begin{align}\n",
    "q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v_\\pi(s') \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "since $v_\\pi(s) = \\sum_{a} \\pi(a|s) q_\\pi(s,a)$\n",
    "\n",
    "\n",
    "#### Bellman: Matrix Form\n",
    "\n",
    "$$ \\begin{align} v = (I - \\gamma\\mathcal{P})^{-1}\\mathcal{R} \\end{align}$$\n",
    "\n",
    "where $\\mathcal{P}$ is the transition matrix \n",
    "$$ \\begin{align}\\mathcal{P} = \\begin{pmatrix} p_{11} & \\dots & p_{1n}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n1} & \\dots & p_{nn} \\end{pmatrix} \\end{align} $$\n",
    "and $\\mathcal{R}$ the reward matrix\n",
    "\n",
    "\n",
    "#### Optimal Policy\n",
    "\n",
    "$$ \\begin{align} v_*(s) &= \\max_\\pi v_\\pi(s) \\quad \\forall \\: s \\in \\mathcal{S} \\\\\n",
    "q_*(s,a) &= \\max_\\pi \\underbrace{\\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = t \\right]}_{q_\\pi(s,a)} \\end{align} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_policy(policy):\n",
    "    P = trans_matrix_for_policy(policy)\n",
    "    P[-1] = 0\n",
    "    # (P, r and gamma already given)\n",
    "    v = np.linalg.solve(np.eye(n_states) - gamma * P, r)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "\n",
    "```python\n",
    "for s in states:\n",
    "    v_temp = V[s]\n",
    "    V[s] = np.sum([pi[s,a] * np.sum([p[s,a,s1] * (r[s,a,s1] + gamma * V[s1]) for s1 in states]) for a in actions])\n",
    "    delta = max(delta, np.abs(v_temp - V[s]))\n",
    "```\n",
    "until $\\Delta < \\theta$.\n",
    "\n",
    "<img src=\"notes/image/algorithms/policy_iteration.png\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contraction\n",
    "\n",
    "Using the infinity norm, we can use the bellman equation as a contraction\n",
    "\n",
    "$$ \\begin{align} (T^\\pi v)(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma v(s') \\right] \\end{align} $$\n",
    "\n",
    "proof involves property of maximum norm ($\\infty$-norm) see [here](slides/Lecture%2003.pdf)\n",
    "\n",
    "\n",
    "#### Asynchronous DP\n",
    "\n",
    "* In-place\n",
    "* Prioritized sweeping (e.g. Bellman error $|v_{bel} - v|) e.g. via queue\n",
    "* real-time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "\n",
    "Estimation of black box $f$ w.r.t. some distribution which satisfies\n",
    "\n",
    "* Samples i.i.d.\n",
    "* $f$ encodes environment and returns sequence of experience\n",
    "\n",
    "### MC in RL\n",
    "\n",
    "* **Learn** value function $v()$ from *experience* (env or sim)\n",
    "* **Discover** optimal policies\n",
    "* **Blackbox view** ($p(s'|s,a)$ and $r(s,a,s')$ isnt required)\n",
    "\n",
    "<img src=\"notes/image/blackbox.png\" width=\"50%\">\n",
    "\n",
    "#### MC principle\n",
    "\n",
    "* Divide into (terminating!) episodes\n",
    "* estimate $v(\\cdot)$ and update each **episode** (instead of each step like DP)\n",
    "* TODO: cant use $v(\\cdot)$ to improve $\\pi$\n",
    "* $v(\\cdot)$ is approx. for $s, \\pi$ pair\n",
    "    * first visit\n",
    "    * every visit\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC properties\n",
    "\n",
    "* Independent estimate for $v(\\cdot)$\n",
    "* Compute time is independent of $|\\mathcal{S}|$\n",
    "* Possible to focus on relevant states and ignore the value of others\n",
    "\n",
    "Need to estimate \n",
    "$$ \\begin{align} \\pi'(s) = \\argmax_a q_\\pi(s,a) \\end{align} $$\n",
    "\n",
    "And similar\n",
    "$$ \\begin{align} q_\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t|S_t = s, A_t = a \\right] \\end{align} $$\n",
    "by averaging returns following first visit to that state-action pair [Warning p.20](slides/Lecture%2004.pdf)\n",
    "\n",
    "#### MC control\n",
    "\n",
    "<img src=\"notes/image/MC_control.png\" width=\"50%\">\n",
    "\n",
    "* converges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### On-policy\n",
    "* learn about policy currently used to generate experience\n",
    "\n",
    "<img src=\"notes/image/algorithms/MC/First-visit_MC_control.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "### Off-policy\n",
    "* evaluate and improve a policy that is different from the one used for genereating episodes\n",
    "* *target* policy $\\pi$ and *behavior* policy $\\mu$\n",
    "* it holds that $\\mu$ generates behavior which covers $\\pi$ \n",
    "$$ \\begin{align} \\pi(a|s) > 0 \\implies \\mu(a|s) > 0 \\: \\forall_{s,a} \\end{align} $$\n",
    "*compute MC estimates from the trajectories generated by $\\mu$ but make adjustments such that we obtain estimates compatible with $\\pi$*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "* Target distribution $p(x)$\n",
    "* proposal distribution $q(x)$\n",
    "\n",
    "$$ \\begin{align} \\mathbb{E}_{p(x)}[f(x)] &= \\int f(x) p(x) \\mathrm{d}x = \\int f(x) \\frac{p(x)}{q(x)} q(x) \\mathrm{d}x = \\mathbb{E}_{q(x)} \\left[ f(x) \\frac{p(x)}{q(x)} \\right] \\\\\n",
    "&\\approx \\frac{1}{L} \\sum_{l=1}^L f(x_l) \\underbrace{\\frac{p(x_l)}{q(x_l)}}_{w_l}, \\quad \\text{with samples} \\: x_l \\overset{i.i.d.}{\\sim} q(x) \\end{align} $$\n",
    "\n",
    "See further [here](slides/Lecture%2004.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "* combines **sampling** [MC] with **bootstrapping** (update based in part on existing estimate) [DP] see page 9 [here](slides/Lecture%2005.pdf)\n",
    "\n",
    "### Incremental Monte Carlo\n",
    "\n",
    "$$ \\begin{align} V(s) \\leftarrow V(s) + \\underbrace{\\frac{1}{n(s) + 1}}_{\\alpha} [\\underbrace{G_t}_{\\text{MC target}} - V(s)] \\end{align} $$ \n",
    "where $\\alpha$ could be given as a relation with respect to the number of first visits of states $s$ \n",
    "\n",
    "Simplest temporal difference update TD(0)\n",
    "$$ target = R_{t+1} + \\gamma V(S_{t+1})$$\n",
    "\n",
    "\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| TD | MC |\n",
    "| --- | --- |\n",
    "| learn before terminal | unbiased |\n",
    "| non-episodic or episodic task | - |\n",
    "| lower variance | - |\n",
    "| bias - TD updates exploit the Markov property| |\n",
    "\n",
    "both converge but these value function are potentially different. See example [here](slides/Lecture%2005.pdf). Marriage of MC and TD [here](#unifying)\n",
    "\n",
    "<img src=\"notes/image/graphs/comparison_square.png\" alt=\"Sarsa Q Expected Sarsa\" width=\"60%\">\n",
    "<!-- <img src=\"notes/image/TD_MC_DP.png\" width=\"40%\"> -->\n",
    "\n",
    "### TD for control\n",
    "estimate $Q$ instead of $V$\n",
    "\n",
    "$$ Q(S_t, A+t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\underbrace{Q(S_{t+1})}_{\\text{Only this part changes}} - Q(S_t, A_t)] $$\n",
    "\n",
    "<img src=\"notes/image/graphs/Sarsa_Q_Exp.png\" width=\"50%\">\n",
    "\n",
    "* change $\\pi$ toward greediness wrt $Q_\\pi$\n",
    "* use soft policies, e.g. $\\varepsilon$-greedy\n",
    "* converges to $Q_*$\n",
    "\n",
    "#### SARSA (on-policy)\n",
    "<img src=\"notes/image/algorithms/SARSA.png\" width=\"80%\">\n",
    "\n",
    "#### Q-learning (off-policy)\n",
    "* Target policy $\\pi$ is greedy $$\\pi(a) = \\argmax_a Q(s,a)$$\n",
    "* Behavior policy $\\mu$ is soft plicy, e.g. $\\varepsilon-$ greedy\n",
    "<img src=\"notes/image/algorithms/Qlearning.png\" width=\"80%\">\n",
    "\n",
    "#### Comparison of Q-learning and SARSA\n",
    "\n",
    "<img src=\"notes/image/Sarsa_vs_Q.png\" width=\"80%\">\n",
    "\n",
    "#### Extensions\n",
    "\n",
    "##### Expected SARSA\n",
    "\n",
    "Both on- and off-policy\n",
    "\n",
    "\n",
    "##### Maximization bias\n",
    "\n",
    "* positive maximization bias\n",
    "\n",
    "<img src=\"notes/image/examples/max_bias.png\" width=\"30%\">\n",
    "\n",
    "##### Double Q-learning\n",
    "\n",
    "* Divide into $Q_1$ and $Q_2$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & Planning\n",
    "\n",
    "### Model\n",
    "* Anything the agent can use like an environment\n",
    "    * Distribution model (ideal)\n",
    "    * Sample model (approx.) \n",
    "\n",
    "### Planning\n",
    "* Uses model, e.g. $\\text{model} \\overset{\\text{planning}}{\\to}\\text{policy}$\n",
    "* Types\n",
    "  * state-space planning\n",
    "    * search through the state space\n",
    "    * e.g. classical DP, heustic search\n",
    "  * plan-space (IGNORE, not done in lecture)\n",
    "\n",
    "\n",
    "<img src=\"notes/image/RL_cycle.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "| Direct | Indirect |\n",
    "| --- | --- |\n",
    "| simpler | fuller use of experience |\n",
    "| not affected by bad model | better policy with fewer env it |\n",
    "\n",
    "* can occur *simultaneously* and in *parallel*\n",
    "\n",
    "\n",
    "\n",
    "### Dyna architecture\n",
    "\n",
    "<img src=\"notes/image/algorithms/DynaQ.png\" width=\"80%\">\n",
    "\n",
    "#### DYna-Q+\n",
    "\n",
    "Uses exploration bonus\n",
    "\n",
    "$$ \\begin{align} R + \\kappa \\sqrt{\\tau} \\end{align} $$\n",
    "\n",
    "\n",
    "### Prioritized sweeping\n",
    "\n",
    "* which $s$ or $(s,a)$ is nice in planning\n",
    "  * maintain a queue $(s,a)$ who change a lot\n",
    "  * when a new backup occurs insert predecessors according to their priorities\n",
    "  * always perform backups from first in queue\n",
    "\n",
    "  <img src=\"notes/image/algorithms/PrioritizedSweeping.png\" width=\"80%\">\n",
    "  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation based search\n",
    "* Planning after encountering a new state $S_t$ (instead of improving overall policy)\n",
    "\n",
    "#### Forward search\n",
    "* build a search tree with the current state $s_t$\n",
    "\n",
    "<img src=\"notes/image/forward_search.png\" width=\"50%\">\n",
    "\n",
    "#### Rollout\n",
    "\n",
    "TODO: simulate from this point onwards, the model therefore is\n",
    "\n",
    "$$ \\begin{align} \\{s_t^k, A_t^k, R_{t+1}^k,\\dots, S_T^k\\}_{k=1}^K \\sim \\mathcal{M}_v \\end{align} $$\n",
    "\n",
    "Then we can apply model-free RL to simulate episode, e.g\n",
    "* MC control $\\to$ [MC search](#mc-search)\n",
    "* SARSA $\\to$ TD search\n",
    "\n",
    "#### MC search\n",
    "\n",
    "##### Simple\n",
    "\n",
    "* Model $\\mathcal{M}_v$, policy $\\pi$\n",
    "* For each action $a \\in \\mathcal{A}$\n",
    "$$ \\begin{align} \\{s_t^k, a_t, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k \\dots, S_T^k\\}_{k=1}^K \\sim \\mathcal{M}_v \\end{align} $$\n",
    "\n",
    "evaluate by mean return\n",
    "\n",
    "$$ \\begin{align} Q(s_t, a_t) = \\frac{1}{N} \\sum_{k=1}^K G_t \\overset{P}{\\to} q_\\pi(s_t, a)  \\end{align} $$\n",
    "\n",
    "$$ a_t = \\argmax_{a \\in \\mathcal{A}} Q(s_t, a)$$\n",
    "\n",
    "\n",
    "##### Tree\n",
    "\n",
    "* Selection\n",
    "* Expansion\n",
    "* Simulation\n",
    "  * Tree policy (improves)\n",
    "  * Rollout policy (fixed)\n",
    "* Backup\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "* Highly selective best-first search\n",
    "* Evaluates states dynamically\n",
    "* Sampling -> No curse of dimensionality\n",
    "* Works for 'black-box'\n",
    "* Efficient, parallelisable\n",
    "\n",
    "\n",
    "<img src=\"notes/image/MC_tree_search.png\" width=\"50%\">\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Value function approximation\n",
    "\n",
    "$$ \\hat{v}_\\pi(s, \\mathbf{w}) \\approx v_\\pi(s) $$\n",
    "\n",
    "with $\\mathbf{w} \\in \\mathbb{R}^d \\quad d \\ll |\\mathcal{S}|$ as parameter vector\n",
    "\n",
    "Features can be diverse, e.g.\n",
    "* Linear combinations\n",
    "  * state aggregation\n",
    "  * tile coding\n",
    "  * polynomial basis\n",
    "  * Fourier basis\n",
    "  * RBF\n",
    "* neural networks\n",
    "* decision trees\n",
    "* non-parametric methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SGD\n",
    "\n",
    "Approx the true gradient\n",
    "$$ \\begin{align} \\nabla f(\\mathbf{w}) = \\begin{pmatrix} \\frac{\\partial f(\\mathbf{w})}{\\partial w_1} \\\\ \\frac{\\partial f(\\mathbf{w})}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial f(\\mathbf{w})}{\\partial w_d} \\end{pmatrix}\\end{align} $$\n",
    "with the update rule\n",
    "$$ \\begin{align} \\mathbf{w}_{t+1} &= \\mathbf{w}_t - \\alpha_t \\sum_{n=1}^{N} (\\nabla \\mathcal{L}_n)(\\mathbf{w}_t) \\\\ \n",
    "&= \\mathbf{w}_t - \\frac{1}{2} \\alpha_t \\nabla \\underbrace{\\left[ v_\\pi(S_t) - \\hat{v}(S_t, \\mathbf{w_t}) \\right]^2}_{\\text{squared sample error}} \\\\\n",
    "&= \\mathbf{w}_t + \\alpha_t \\left[ v_\\pi(S_t) - \\hat{v}(S_t, \\mathbf{w_t}) \\right]\\nabla \\hat{v}(S_t, \\mathbf{w_t}) \\\\\n",
    "&= \\mathbf{w}_t + \\alpha_t \\left[ U_t - \\hat{v}(S_t, \\mathbf{w_t}) \\right]\\nabla \\hat{v}(S_t, \\mathbf{w_t})\n",
    "\\end{align}$$\n",
    "\n",
    "Where $U_t$ might be a noisy bootstrapped approximation of the true value, e.g.\n",
    "* MC $U_t = G_t$\n",
    "* TD(0) $U_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w_t) $\n",
    "* $TD(\\lambda) \\: U_t = G^\\lambda_t$\n",
    "\n",
    "MC is unbiased by Definition\n",
    "$$ \\begin{align} \\mathbb{E}[U_t | S_t = s] = \\mathbb{E}[G_t|S_t = s] = v_\\pi(S_t) \\end{align} $$\n",
    "\n",
    "For more details on Training data $\\mathcal{D}$ see [Slide 17](slides/Lecture%2007.pdf  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case linear methods\n",
    "\n",
    "$$ \\begin{align} \\mathbf{x}(s) &= \\begin{pmatrix} x_1(s) \\\\ x_2(s) \\\\ \\vdots \\\\ x_d(s) \\end{pmatrix} \\\\\n",
    "\\hat{v}(s, \\mathbf{w} &= \\mathbf{w}^T \\mathbf{x}(s) \\\\\n",
    "\\nabla \\hat{v}(s, w) &= \\mathbf{x}(s) \\end{align}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control with function approximation\n",
    "\n",
    "Generalized policy iteration (GPI), where\n",
    "$$ \\begin{align} \\hat{q}(s,a,\\mathbf{w}) \\approx q_\\pi(s,a) \\end{align} $$\n",
    "\n",
    "Again substitute $U_t$\n",
    "\n",
    "$$ \\begin{align} \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\left[ U_t - \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\right] \\nabla \\hat{q}(S_t, A_t, \\mathbf{w})\\end{align} $$\n",
    "\n",
    "With examples like\n",
    "* MC $U_t = G_t$\n",
    "* One-Step Sarsa $U_t = R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w})$\n",
    "\n",
    "#### Linear case\n",
    "\n",
    "$$ \\begin{align} \\hat{q}(s,a,\\mathbf{w}) &= \\mathbf{w}^T \\mathbf{x}(s,a) \\\\\n",
    "\\nabla \\hat{q}(s,a,\\mathbf{w}) &= \\mathbf{x}(s,a) \\end{align} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning\n",
    "\n",
    "Composition of many functions, non-linear activation functions $h_k$\n",
    "\n",
    "Weight sharing\n",
    "* RNN\n",
    "* CNN\n",
    "\n",
    "#### Naive deep Q-learning\n",
    "\n",
    "* $Q(s, a)$ is a neural network\n",
    "\n",
    "##### Problems\n",
    "\n",
    "1. Non i.i.d\n",
    "  * trajectories\n",
    "  * samples are correlated\n",
    "2. Policy changes rapidly\n",
    "  * may oszillate\n",
    "3. Reward range is unknown\n",
    "  * grad can be large\n",
    "  * instabilities during back-propagation\n",
    "4. Maximization bias\n",
    "\n",
    "##### Deep Q-Networks\n",
    "Mitigate the problems\n",
    "* Experience replay\n",
    "* Target network\n",
    "* Reward clipping, i.e. $r \\in [-1, 1]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying\n",
    "\n",
    "MC and TD can be related via\n",
    "\n",
    "$$ \\begin{align} G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n}) \\end{align} $$\n",
    "\n",
    "<img src=\"notes/image/graphs/N_Step_TD.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error-reduction property\n",
    "\n",
    "$$ \\begin{align} \\underbrace{\\max_s|\\mathbb{E}_\\pi[G_{t:t+n}|S_t = s] - v_\\pi(s)|}_{\\text{Maximum error using n-step return}} \\leq \\underbrace{\\gamma^n \\max_s |V_{t+n-1}(s) - v_\\pi(s)|}_{ \\text{Maximum error using V}} \\end{align} $$\n",
    "\n",
    "* Wait until time $t+n$ and then\n",
    "\n",
    "$$ \\begin{align} V_{t+n}(S_t) = V_{t+n-1}(S_t) + \\alpha \\left[ G_{t:t+n} - V_{t+n-1}(S_t) \\right] \\end{align} $$\n",
    "\n",
    "#### Unifying with Q\n",
    "* For Action-Value substitute $V$ with $Q$, e.g\n",
    "\n",
    "$$ \\begin{align} Q_{t+n}(S_t, A_t) &= Q_{t+n-1}(S_t, A_t) + \\alpha \\left[ G_{t:t+n} - Q_{t+n-1}(S_t, A_t) \\right] \\\\\n",
    " G_{t:t+n} &= R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}) \\\\\n",
    " G_{t:t+n, exp} &= R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\sum_a \\pi(a| S_{t+n})Q_{t+n-1}(S_{t+n}, a) \\end{align} $$\n",
    "\n",
    " #### Importance Sampling II\n",
    "\n",
    " Recall [here](#importance-sampling)\n",
    "\n",
    " $$ \\begin{align} \\rho_{t:h} = \\prod_{k=t}^{\\min(h,T-1)} \\frac{\\pi(A_k|S_k)}{\\mu(A_k)|S_k} \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go Big or go Home (Unifying II)\n",
    "\n",
    "Large state spaces\n",
    "\n",
    "### Lambda return\n",
    "\n",
    "Continuos version of n-Step Unifying\n",
    "\n",
    "$$ \\begin{align} G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1}G_{t:t+n} \\end{align} $$\n",
    "\n",
    "### Eligibility Traces \n",
    "\n",
    "* Heuristic is to use **Frequency** and **Recency**. E.g. model via decay\n",
    "$$ \\begin{align} \\forall s: e(s) &\\leftarrow \\gamma \\lambda e(s) \\\\\n",
    "e(S_t) &\\leftarrow e(S_t) + 1 \\end{align} $$\n",
    "\n",
    "#### Forward view\n",
    "\n",
    "* Only possible for terminated sequences\n",
    "\n",
    "#### Backward view\n",
    "\n",
    "* Works for incomplete sequences\n",
    "* Eligibility via $\\delta$\n",
    "$$ \\begin{align} \\delta_t = R_{t+1} + \\gamma V(S_{t+1})- V(S_t) \\\\\n",
    "\\forall s: V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s) \\end{align} $$\n",
    "\n",
    "Combined this evaluates to\n",
    "\n",
    "$$ \\begin{align} \\delta &= R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\\\\n",
    "e &\\leftarrow \\gamma \\lambda \\mathbf{e} \\nabla \\hat{v}(S_t, \\mathbf{w}) \\\\\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} + \\alpha \\delta \\mathbf{e} \\end{align} $$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based RL\n",
    "\n",
    "| Advantages | Cons |\n",
    "| --- | --- |\n",
    "| can converge to det. policy | local rather then global optimum |\n",
    "| high-dim and cont. action-spaces | high variance |\n",
    "| can learn stochastic policies | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align} \\pi_* = \\pi(a|s, \\mathbf{\\theta}_*) \\end{align} $$\n",
    "with $\\mathbf{\\theta}_* = \\argmax_\\theta J(\\theta)= \\argmax_\\theta \\mathbb{E}_{\\pi_\\theta}[G_0]$\n",
    "\n",
    "therefore we train via **policy gradient**\n",
    "\n",
    "$$ \\begin{align} \\nabla J(\\theta) = \\begin{pmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_0} \\\\ \\vdots \\\\ \\frac{\\partial j(\\theta)}{\\partial \\theta_d} \\end{pmatrix} \\end{align} $$\n",
    "\n",
    "### Score function\n",
    "\n",
    "* Assume $\\pi_\\theta$ is differentiable\n",
    "\n",
    "$$ \\begin{align} \\nabla_\\theta \\pi_\\theta(a|s) &= \\pi_\\theta(a|s) \\frac{\\nabla_\\theta \\pi_\\theta(a|s)}{\\pi_\\theta(a|s)} \\\\\n",
    "&= \\pi_\\theta(a|s) \\underbrace{\\nabla_\\theta \\log \\pi_\\theta(a|s)}_{\\text{score function}} \\end{align} $$\n",
    "\n",
    "#### Softmax policy\n",
    "\n",
    "$$ \\begin{align} \\pi_\\theta = \\frac{e^{\\phi(s,a)^T \\theta}}{\\sum_{a'} e^{\\phi(s,a')^T\\theta}} \\end{align} $$\n",
    "Score \n",
    "$$ \\begin{align} \\nabla_\\theta \\pi_\\theta(a|s) &= \\\n",
    "\\phi(s,a) - \\mathbb{E}_{\\pi_\\theta}[\\phi(s, \\cdot)] \\end{align} $$\n",
    "\n",
    "#### Gaussian policy\n",
    "$$ \\begin{align} a \\sim \\mathcal{N}(\\mu(s), \\sigma^2) \\end{align} $$\n",
    "Score\n",
    "$$ \\begin{align} \\nabla_\\theta \\pi_\\theta(a|s) &= \\frac{(a - \\mu(s))\\phi(s)}{\\sigma^2} \\end{align} $$\n",
    "\n",
    "## Policy gradient Theorem\n",
    "\n",
    "$$ \\begin{align} \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ q_\\pi(s,a) \\nabla_\\theta \\log \\pi(a|s, \\theta) \\right] \\end{align} $$\n",
    "from which the update rule \n",
    "$$ \\begin{align} \\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\log \\pi(A_t|S_t, \\theta) \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "\n",
    "* Update $\\theta$ by SGA\n",
    "\n",
    "#### with Baseline\n",
    "\n",
    "reduces Variance without adding bias\n",
    "\n",
    "$$ \\begin{align} \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ (q_\\pi(s,a) - \\underbrace{b(s)}_{\\text{new part}}) \\nabla_\\theta \\log \\pi(a|s, \\theta) \\right] \\end{align} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-critic\n",
    "\n",
    "<img src=\"notes/image/cycles/actor-critic.png\" width=\"50%\">\n",
    "\n",
    "| Actor-Critic | baseline |\n",
    "| --- | --- |\n",
    "| Uses $V$ for parameters $w$ | uses $V$ as baseline |\n",
    "| Bias through bootstrapping | No bias |\n",
    "\n",
    "but AC substantially reduces variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
